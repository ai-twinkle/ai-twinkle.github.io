{
  "site": {
    "title": "Twinkle AI",
    "subtitle": "Open-source Traditional Chinese LLM community from Taiwan"
  },
  "actions": {
    "join_discord": "Discord",
    "explore_projects": "Explore projects",
    "explore_models": "Explore Models",
    "huggingface": "Hugging Face",
    "explore_datasets": "Explore Datasets"
  },
  "nav": {
    "leaderboard": "Leaderboard",
    "models": "Models",
    "community": "Community",
    "resources": "Resources",
    "education": "Education",
    "education_desc": "Tutorials, workshops, and promotional content",
    "news": "News & Events",
    "news_desc": "Latest news, events, and announcements",
    "media": "Media",
    "media_desc": "Press releases, media kit, and resources"
  },
  "pages": {
    "coming_soon": "Coming soon...",
    "coming_soon_badge": "Coming Soon"
  },
  "news": {
    "description": "Latest news, events, and announcements"
  },
  "education": {
    "description": "Tutorials, workshops, and promotional content"
  },
  "media": {
    "description": "Press releases, media kit, and resources"
  },
  "home": {
    "badge": "ðŸŒŸ Taiwan open-source LLM community",
    "titleStart": "Light up the starry sky of ",
    "titleHighlight": "Traditional Chinese AI",
    "titleEnd": "",
    "lead": "Twinkle AI is a community focused on building open-source Traditional Chinese language models. We bring together passionate contributors to implement, share, and drive the adoption of AI technologies in Taiwan.",
    "about": {
      "title": "About Twinkle AI: Why Are We Doing This?",
      "content": "AI is moving faster than a scooter in Taipei rush hour. While global models are geniuses, they often talk to us using a vocabulary that sounds like itâ€™s from across the seaâ€”filled with terms that just don't belong in Taiwan. At Twinkle AI, weâ€™re fixing that cultural mismatch to build AI that actually speaks our language.\n\nWe believe Language is Life. If AI doesn't get our local vibe, our unique way of speaking will just fade into the algorithm. Since 2025, weâ€™ve been curating data and fine-tuning models so they actually \"get\" usâ€”natural, precise, and distinctly local."
    },
    "models": {
      "title": "Training Traditional Chinese Models",
      "description": "Weâ€™re a bit obsessed with making AI thatâ€™s both smart and lightweightâ€”because your hardware shouldnâ€™t have to break a sweat just to understand the local culture.",
      "items": [
        {
          "title": "T1 Model",
          "description": "Built on Gemma-3-4B with a local tune-up. It handles our logic surprisingly well for a model of its size."
        },
        {
          "title": "Formosa-1",
          "description": "Based on Llama-3.2-3B. Our first real step toward fixing the \"translation\" feel in AI."
        },
        {
          "title": "Twinkle Voice",
          "description": "A voice-first branch. Because sometimes, your hands deserve a break from typing."
        }
      ]
    },
    "datasets": {
      "title": "Open Source Datasets",
      "content": "High-quality data is everything. We prioritize \"Data Cleansing and Linguistic Calibration\" over sheer volume.\n\nOur sources range from public government archives to local community discussions and literature. We use an automated pipeline to filter out non-local idioms."
    },
    "eval": {
      "title": "Evaluation & Benchmarking",
      "content": "International benchmarks often miss the nuances of local life. We built our own ecosystem to bridge the gap between global AI and our actual lived experience.",
      "items": [
        {
          "title": "Twinkle Eval",
          "description": "A benchmark suite for local law, geography, and general knowledge."
        },
        {
          "title": "Eval-analyzer",
          "description": "A diagnostic tool that spots weird grammar or cultural mismatches so we can refine our models."
        },
        {
          "title": "Leaderboard",
          "description": "A visual platform to see how models actually perform in a local context at a glance."
        }
      ]
    },
    "teaser": {
      "education": "Explore Education Programs",
      "media": "Visit Media Zone"
    }
  },
  "features": {
    "optimized": {
      "title": "Optimized for Traditional Chinese",
      "description": "We collect Traditional Chinese datasets and fine-tune LLaMA-like models to better fit Taiwan's local culture and usage."
    },
    "open_source": {
      "title": "Open & Collaborative",
      "description": "From datasets to model weights, we keep things open to lower barriers and encourage community contributions."
    },
    "tools": {
      "title": "Evaluation & Tools",
      "description": "We develop Twinkle Eval and TwinRAD to provide efficient and accurate model evaluation and red-team testing frameworks."
    }
  },
  "social": {
    "discord": "Discord",
    "huggingface": "Hugging Face",
    "github": "GitHub"
  },
  "footer": {
    "tagline": "Open-source Traditional Chinese LLM community â€¢ Open â€¢ Collaboration â€¢ Teaching",
    "recommendedPrefix": "You might be interested in our:",
    "copyright": "Â© {year} Twinkle AI",
    "navigation": "Navigation",
    "resources": "Resources",
    "connect": "Connect",
    "open_source": "Open Source"
  },
  "projects": {
    "title": "Open-source projects",
    "lead": "We commit to open-source all our research outputs, including model weights, training data, and evaluation tools.",
    "loading": "Loading...",
    "errorTitle": "Error loading projects:",
    "retry": "Retry",
    "noProjects": "No projects to display",
    "hf": {
      "title": "Hugging Face models",
      "lead": "Popular models by us on Hugging Face.",
      "loading": "Loading models...",
      "errorTitle": "Error loading models:",
      "retry": "Retry",
      "noModels": "No models to display"
    },
    "github": {
      "title": "GitHub repositories",
      "lead": "Top repositories by us on GitHub.",
      "loading": "Loading repositories...",
      "errorTitle": "Error loading repositories:",
      "retry": "Retry",
      "noProjects": "No repositories to display"
    }
  }
}
