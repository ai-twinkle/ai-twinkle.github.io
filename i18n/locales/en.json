{
  "site": {
    "title": "Twinkle AI",
    "subtitle": "Open-source Traditional Chinese LLM community from Taiwan"
  },
  "actions": {
    "join_discord": "Discord",
    "explore_projects": "Explore projects",
    "explore_models": "Explore Models",
    "huggingface": "Hugging Face",
    "explore_datasets": "Explore Datasets",
    "read_more": "Read More",
    "download_press_kit": "Download Press Kit",
    "download_pdf_zh": "Traditional Chinese - PDF",
    "download_pdf_en": "English - PDF"
  },
  "nav": {
    "about": "About Us",
    "leaderboard": "Twinkle Eval Leaderboard",
    "leaderboard_desc": "Model performance rankings and benchmarks",
    "models": "Models",
    "models_desc": "Models trained with Traditional Chinese context at their core.",
    "datasets": "Datasets",
    "datasets_desc": "Refined government, community, and literary archives.",
    "community": "Community",
    "resources": "Resources",
    "education": "Education",
    "education_desc": "Twinkle Podcasts, Midnight Tinkering, and Late-night Reading Groups.",
    "news": "News & Events",
    "news_desc": "Latest news, events, and announcements",
    "media": "Media",
    "media_desc": "Press releases, media kit, and resources"
  },
  "pages": {
    "coming_soon": "Coming soon...",
    "coming_soon_badge": "Coming Soon"
  },
  "home": {
    "badge": "üåü Taiwan open-source LLM community",
    "titleStart": "Illuminating the Future of ",
    "titleHighlight": "Traditional Chinese AI",
    "titleEnd": "",
    "lead": "Twinkle AI is a community focused on building open-source Traditional Chinese language models. We bring together passionate contributors to implement, share, and drive the adoption of AI technologies in Taiwan.",
    "about": {
      "title": "About Twinkle AI: Why Are We Doing This?",
      "description": "Building AI that speaks our language and understands our culture.",
      "content": "AI is moving faster than a scooter weaving through Taipei traffic. While global models are incredibly powerful, they often feel disconnected from our daily lives‚Äîspeaking to us in a way that‚Äôs technically correct but culturally distant. \n\nWe believe Language is Life‚Äîit‚Äôs the soul of our identity. But without AI that understands our local nuance, the way we speak risks being washed away by generic data. At Twinkle AI, we‚Äôre here to bridge that gap. Since 2025, we‚Äôve been curating local data and refining models with a single mission: to build an AI that actually \"gets\" us‚Äîone that feels natural, intuitive, and authentically local."
    },
    "models": {
      "title": "Training Traditional Chinese Models",
      "description": "We‚Äôre a bit obsessed with making AI that‚Äôs both smart and lightweight‚Äîbecause your hardware shouldn‚Äôt have to break a sweat just to understand the local culture.",
      "items": [
        {
          "title": "T1 Model",
          "description": "A fine-tuned Gemma-3-4B built for precision reasoning. It masters local nuances to deliver natural, reliable dialogue without the generic fluff."
        },
        {
          "title": "Formosa-1",
          "description": "The model that started it all. Based on Llama-3.2-3B, it sparked our early growth and proved that compact models can handle Traditional Chinese with a natural flow."
        },
        {
          "title": "Twinkle Voice",
          "description": "A voice-first branch. Because sometimes, your hands deserve a break from typing."
        }
      ]
    },
    "datasets": {
      "title": "Open Source Datasets",
      "content": "Quality beats quantity, every time. Instead of chasing sheer volume, we focus on rigorous curation to ensure our data reflects how people in Taiwan truly speak.\n\nBy curating data from government open data, local community discussions, and Taiwanese literary archives, our pipeline filters out non-local idioms to keep the context accurate. In our world, it‚Äôs always 'Ë≥áË®ä', never '‰ø°ÊÅØ'‚Äîbecause asking for the 'Soccer' score in a London pub is a guaranteed way to lose the room."
    },
    "eval": {
      "title": "Evaluation & Benchmarking",
      "content": "International benchmarks often miss the nuances of local life. We built our own ecosystem to bridge the gap between global AI and our actual lived experience.",
      "items": [
        {
          "title": "Twinkle Eval",
          "description": "A dedicated utility for benchmark execution. Instead of hosting questions, it plugs models into localized datasets like TMMLU+ to automate testing and generate standardized logs."
        },
        {
          "title": "Eval-analyzer",
          "description": "The diagnostic layer for Eval logs. It maps out where a model excels and where it trips up, pinpointing knowledge gaps for precise calibration."
        },
        {
          "title": "Leaderboard",
          "description": "The public dashboard for Eval results. We turn raw data into clear rankings, making it easy to compare how models perform against local benchmarks at a glance."
        }
      ]
    },
    "teaser": {
      "education": "Learning & Tinkering",
      "media": "Latest Sparks",
      "title": "Beyond the Models",
      "description": "Peek behind the scenes at the projects and people shaping our community."
    },
    "highlights": {
        "models": "Meet the Models",
        "datasets": "Explore the Data",
        "education": "Let‚Äôs Learn"
    }
  },
  "about": {
    "vision": {
      "title": "Our Vision",
      "content": "Twinkle AI started with a simple conviction: language is the heartbeat of culture. If future digital tools can't grasp the nuances of how we actually speak, our identity risks being diluted by one-size-fits-all algorithms.\n\nSince early 2025, we‚Äôve been curating local datasets and training models with one clear goal ‚Äî to build an AI that truly 'gets' us and feels like a natural conversation partner."
    },
    "story": {
        "title": "How It All Started",
        "content": "We noticed that AI responses often feel disconnected, largely because they‚Äôre trained on an influx of external data that misses the local perspective. Whether it's legal nuances or cultural context, general-purpose models often miss the mark. That‚Äôs why we decided to start from the ground up, building the first Llama-based model specifically for Taiwan."
    },
    "timeline": {
        "title": "Milestones",
        "events": [
            {
                "date": "2025.01",
                "title": "Founding Llama Taiwan",
                "description": "A small group of us gathered with a single mission: to build a model that speaks our language. This was the birth of the Llama Taiwan community."
            },
            {
                "date": "2025.03",
                "title": "Rebranded to Twinkle AI",
                "description": "We officially became Twinkle AI, symbolizing a community of bright minds coming together to build next-gen models."
            },
            {
                "date": "2025.03",
                "title": "Fineweb-edu-zhtw Project",
                "description": "Partnered with Taiwan Mobile to build educational datasets, sharpening the model's performance on local knowledge."
            },
            {
                "date": "2025.04",
                "title": "Releasing Formosa-1",
                "description": "Collaborated with APMIC and National Center for High-Performance Computing to build Formosa-1, a 3B-parameter model built for the local context."
            },
            {
                "date": "2025.07",
                "title": "Cybersecurity in Action",
                "description": "CyCraft integrated Formosa-1 as a Guardrail for their XecGuard enterprise product."
            },
            {
                "date": "2025.11",
                "title": "Releasing Formosa Vision Dataset",
                "description": "Partnered with the Taiwan Cultural Memory Bank to curate visual datasets, aligning the model's perception with local culture."
            },
             {
                "date": "2025.11",
                "title": "IT Matters Award",
                "description": "Winning the Open Source Contribution Award. This trophy belongs to the community ‚Äî every single one of you. Enjoy!"
            },
            {
                "date": "2026.01",
                "title": "1st Anniversary Meetup Party",
                "description": "Our first in-person meetup. Seeing all that energy in the room gave us the fuel to keep pushing forward."
            }
        ]
    }
  },
  "datasets": {
    "title": "Open Source Datasets",
    "description": "Curated datasets sourced from government records, community discourse, and literary archives, vetted specifically for training.",
    "philosophy": {
        "title": "Data Vetting & Linguistic Alignment",
        "content": "We don‚Äôt just stockpile data; we curate it. Our focus is on data vetting and linguistic alignment. By sourcing from government records, community discourse, and literary archives, and applying automated filters to strip away non-local phrasing, we ensure our models learn from authentic Taiwanese context rather than generic linguistic imports."
    },
    "vision": {
        "title": "Formosa Vision",
        "content": "In collaboration with the Taiwan Cultural Memory Bank, we are building vision datasets that teach AI to decode local cultural symbols ‚Äî from historic trails and military villages to the landscapes of Matsu. It‚Äôs about teaching the model to see the world through a local lens"
    }
  },
  "news": {
    "description": "Latest news, events, and announcements",
    "filters": {
        "all": "All",
        "media": "Media Reports",
        "event": "Events",
        "tech": "Tech Releases",
        "award": "Awards"
    },
    "noNews": "No news to display",
    "press_kit": {
        "title": "Media Resources & Press Kit",
        "description": "To facilitate reporting and citation by media friends, we provide high-resolution materials related to this news. All files are licensed under CC BY 4.0.",
        "press_release": "Press Release",
        "images": "High-Res Images",
        "identity": "Brand Identity"
    }
  },
  "education": {
    "description": "From podcasts to midnight tinkering, we‚Äôre all about getting hands-on. It‚Äôs about sparking curiosity and turning big ideas into actual code.",
    "podcast": {
        "title": "Twinkle Podcast",
        "description": "Real-world developer stories and lessons learned from the trenches."
    },
    "labs": {
        "title": "Midnight Tinkering",
        "description": "Still at your desk at 10 PM? Might as well build something. We‚Äôre doing deep dives into Traditional Chinese models and hands-on implementation."
    },
    "book_club": {
        "title": "Midnight Deep Dives",
        "description": "Taking theory apart to see how it works. We‚Äôre currently tearing through 'Build a Large Language Model' and other essentials."
    },
    "join": "Join on Discord"
  },
  "media": {
    "description": "Press releases, media kit, and resources"
  },
  "features": {
    "optimized": {
      "title": "Optimized for Traditional Chinese",
      "description": "We collect Traditional Chinese datasets and fine-tune LLaMA-like models to better fit Taiwan's local culture and usage."
    },
    "open_source": {
      "title": "Open & Collaborative",
      "description": "From datasets to model weights, we keep things open to lower barriers and encourage community contributions."
    },
    "tools": {
      "title": "Evaluation & Tools",
      "description": "We develop Twinkle Eval and TwinRAD to provide efficient and accurate model evaluation and red-team testing frameworks."
    }
  },
  "social": {
    "discord": "Discord",
    "huggingface": "Hugging Face",
    "github": "GitHub"
  },
  "footer": {
    "tagline": "Open-source Traditional Chinese LLM community ‚Ä¢ Open ‚Ä¢ Collaboration ‚Ä¢ Teaching",
    "recommendedPrefix": "You might be interested in our:",
    "copyright": "¬© {year} Twinkle AI",
    "navigation": "Navigation",
    "resources": "Resources",
    "connect": "Connect",
    "open_source": "Open Source"
  },
  "projects": {
    "title": "Open-source projects",
    "lead": "We commit to open-source all our research outputs, including model weights, training data, and evaluation tools.",
    "loading": "Loading...",
    "errorTitle": "Error loading projects:",
    "retry": "Retry",
    "noProjects": "No projects to display",
    "hf": {
      "title": "Hugging Face models",
      "lead": "Popular models by us on Hugging Face.",
      "loading": "Loading models...",
      "errorTitle": "Error loading models:",
      "retry": "Retry",
      "noModels": "No models to display"
    },
    "github": {
      "title": "GitHub repositories",
      "lead": "Top repositories by us on GitHub.",
      "loading": "Loading repositories...",
      "errorTitle": "Error loading repositories:",
      "retry": "Retry",
      "noProjects": "No repositories to display"
    }
  },
  "models": {
    "title": "Our Models",
    "description": "Trained on Traditional Chinese linguistic context, these models bring reasoning and dialogue closer to the way we actually live and speak",
    "explore_hf": "Explore on Hugging Face",
    "intro": {
        "t1": {
            "title": "T1 Model",
            "description": "A high-speed favorite based on Gemma-3-4B. Deep-rooted Taiwan knowledge makes it the go-to for natural, precise local dialogue."
        },
        "formosa": {
            "title": "Formosa-1",
            "description": "Logic-first model grounded in Llama-3.2-3B. Engineered for deep reasoning and robust answers to complex prompts."
        },
        "voice": {
            "title": "Twinkle Voice",
            "description": "A voice-first branch. Because sometimes, your hands deserve a break from typing."
        }
    },
    "eval": {
        "title": "Evaluation System",
        "description": "To quantify how deeply a model understands Taiwan, we built a specialized evaluation tool that measures cultural nuances.",
        "leaderboard_action": "Twinkle Eval Leaderboard"
    },
    "table": {
      "name": "Name",
      "version": "Version",
      "parameters": "Parameters",
      "quantised": "Quantised Versions",
      "model_type": "Model Type",
      "context_length": "Context Length"
    }
  },
  "notFound": {
    "title": "404",
    "heading": "Page not found",
    "message": "The page you requested is gone, or never existed.",
    "home": "Back to home"
  }
}
